{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LdQwKWW14j5n",
        "fCPZpmjUoPyn",
        "zwgOlSIfoURM",
        "VmvvbZtMp_v2",
        "ul069bl6rIJq",
        "9fKJpSBwtCTW",
        "ok1OJdHZxrFr",
        "ZiIx6NJPodPN"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b75d9936887b4d1d8e20b442659eebde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b906bf27c8c24e97b57d234c3991ca95",
              "IPY_MODEL_28c59e3b317d4e418b9b25b8075f7f2c",
              "IPY_MODEL_ea5cb281e96244f585f2155041e8f95a"
            ],
            "layout": "IPY_MODEL_bb6e3e23eb9c425fbf0ce5eae5aa1c70"
          }
        },
        "b906bf27c8c24e97b57d234c3991ca95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7855351ddf54e9994e111646239969c",
            "placeholder": "​",
            "style": "IPY_MODEL_187b7759710b4850a7f76154833af0ea",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "28c59e3b317d4e418b9b25b8075f7f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e24fa6597794d8192e2bc1cc6d9d829",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c99fcc2e6bdb4191b6198a55a5d173fe",
            "value": 2
          }
        },
        "ea5cb281e96244f585f2155041e8f95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff711eda99564122a60890ec17b2b964",
            "placeholder": "​",
            "style": "IPY_MODEL_11fe046195d54ae5873328cbc47966a7",
            "value": " 2/2 [01:46&lt;00:00, 48.43s/it]"
          }
        },
        "bb6e3e23eb9c425fbf0ce5eae5aa1c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7855351ddf54e9994e111646239969c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187b7759710b4850a7f76154833af0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e24fa6597794d8192e2bc1cc6d9d829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99fcc2e6bdb4191b6198a55a5d173fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff711eda99564122a60890ec17b2b964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11fe046195d54ae5873328cbc47966a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VmhdyoLTMbml",
        "outputId": "a1ec5961-0b0e-44b5-d8c9-2812514136d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.52.4\n",
        "!pip install -q bitsandbytes==0.46.0\n",
        "!pip install -q accelerate==1.7.0\n",
        "!pip install -q langchain==0.3.25\n",
        "!pip install -q langchainhub==0.1.21\n",
        "!pip install -q langchain-chroma==0.2.4\n",
        "!pip install -q langchain_experimental==0.3.4\n",
        "!pip install -q langchain-community==0.3.24\n",
        "!pip install -q langchain_huggingface==0.2.0\n",
        "!pip install -q python-dotenv==1.1.0\n",
        "!pip install -q pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraris"
      ],
      "metadata": {
        "id": "LdQwKWW14j5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Thư viện học sâu phổ biến, dùng cho xử lý tensor và tăng tốc bằng GPU.\n",
        "# Ví dụ: tensor = torch.tensor([1.0, 2.0]).to(\"cuda\")\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "# Cấu hình lượng tử hóa (quantization) mô hình giúp giảm bộ nhớ khi load mô hình lớn.\n",
        "\"\"\"\n",
        "Các tham số:\n",
        "-----------\n",
        "bnb_4bit_compute_dtype : torch.dtype\n",
        "    Kiểu dữ liệu được sử dụng trong quá trình tính toán sau khi mô hình đã được lượng tử hóa.\n",
        "    Ở đây sử dụng torch.float16 để cân bằng giữa hiệu suất và độ chính xác.\n",
        "\n",
        "bnb_4bit_quant_type : str\n",
        "    Phương pháp lượng tử hóa 4-bit. 'nf4' (Normal Float 4) là kỹ thuật lượng tử hóa tiên tiến giúp mô hình giữ được độ chính xác tốt hơn so với các kỹ thuật cũ như 'fp4'.\n",
        "\n",
        "bnb_4bit_use_double_quant : bool\n",
        "    Nếu đặt là True, sẽ sử dụng double quantization – nghĩa là dữ liệu đầu tiên được lượng tử hóa sang 8-bit trước, sau đó tiếp tục lượng tử hóa sang 4-bit.\n",
        "    Điều này giúp giảm hơn nữa kích thước mô hình mà vẫn giữ được độ chính xác.\n",
        "\n",
        "load_in_4bit : bool\n",
        "    Nếu đặt là True, mô hình sẽ được tải vào ở định dạng 4-bit, giúp giảm tiêu thụ bộ nhớ đáng kể khi làm việc với các mô hình lớn như LLaMA, GPT, v.v.\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# - AutoTokenizer: tự động tải tokenizer tương ứng với mô hình.\n",
        "# - AutoModelForCausalLM: tải mô hình ngôn ngữ dạng sinh (GPT-like).\n",
        "# - pipeline: tạo pipeline inference nhanh cho các task NLP như text-generation, QA...\n",
        "# Ví dụ:\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "# text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "# Dùng pipeline: khi bạn cần nhanh, demo, ít kiểm soát → tiện lợi, rất thích hợp cho LangChain.\n",
        "# Gọi tokenizer + model: khi bạn muốn kiểm soát chi tiết, dùng cho R&D hoặc production nghiêm túc.\n",
        "\n",
        "\"\"\"\n",
        "LangChain là một framework mã nguồn mở giúp xây dựng các ứng dụng dựa trên mô\n",
        "hình ngôn ngữ lớn (LLMs) — như chatbot, hệ thống hỏi đáp (RAG), tóm tắt, tác\n",
        "nhân (agent), và nhiều hơn nữa.\n",
        "\n",
        "Nó kết nối LLM với dữ liệu ngoài và công cụ thực tế, giúp mô hình \"thông minh\n",
        "hơn\" và \"hữu dụng hơn\" thay vì chỉ sinh văn bản tĩnh.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# Tạo hàm embedding từ mô hình trên HuggingFace, dùng trong các vector database.\n",
        "# Ví dụ:\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "# Đóng gói HuggingFace pipeline (text-gen) thành một LangChain LLM để xây chain.\n",
        "# Ví dụ:\n",
        "# llm = HuggingFacePipeline(pipeline=text_gen)\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "# Lưu lại toàn bộ cuộc hội thoại để sử dụng trong các bước tiếp theo.\n",
        "# Ví dụ:\n",
        "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "# Quản lý lịch sử tin nhắn người dùng và hệ thống theo định dạng LangChain.\n",
        "# Ví dụ:\n",
        "# history = ChatMessageHistory()\n",
        "# history.add_user_message(\"Xin chào\"); history.add_ai_message(\"Chào bạn!\")\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "# Dùng để đọc file PDF và file văn bản thuần thành các tài liệu LangChain.\n",
        "# Ví dụ:\n",
        "# docs = PyPDFLoader(\"tài_liệu.pdf\").load()\n",
        "# txt_docs = TextLoader(\"tài_liệu.txt\").load()\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "# Tạo chuỗi hỏi đáp có sử dụng retriever (vector search) và memory hội thoại.\n",
        "# Ví dụ:\n",
        "# qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "# Chia nhỏ văn bản theo ngữ nghĩa (semantic), dựa vào embedding thay vì chỉ đếm ký tự.\n",
        "# Ví dụ:\n",
        "# chunker = SemanticChunker(embedding_model=embeddings)\n",
        "# chunks = chunker.split_documents(docs)\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "# Vector store Chroma – lưu trữ và tìm kiếm vector đã được embedding.\n",
        "# Ví dụ:\n",
        "# db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Cắt văn bản thành các đoạn nhỏ, có thể chồng lặp (overlap), đảm bảo giữ ngữ cảnh.\n",
        "# Ví dụ:\n",
        "# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "# small_docs = splitter.split_documents(docs)\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "# Một bước trung gian truyền nguyên dữ liệu không thay đổi – dùng để debug pipeline.\n",
        "# Ví dụ:\n",
        "# passthrough = RunnablePassthrough()\n",
        "# result = passthrough.invoke(\"test\")  # -> \"test\"\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# Dùng để trích xuất đầu ra dạng chuỗi từ LLM.\n",
        "# Ví dụ:\n",
        "# parser = StrOutputParser()\n",
        "# output = parser.invoke({\"text\": \"Xin chào\"})  # -> \"Xin chào\"\n",
        "\n",
        "from langchain import hub\n",
        "# Truy cập các chain mẫu được chia sẻ trên LangChain Hub như RAG, chatbot, v.v.\n",
        "# Ví dụ:\n",
        "# chain = hub.pull(\"rlm/rag-prompt\")\n"
      ],
      "metadata": {
        "id": "5NptL8YfM3GY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start here"
      ],
      "metadata": {
        "id": "sVxqnjW8nZ0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load file documents"
      ],
      "metadata": {
        "id": "fCPZpmjUoPyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1Wuq0COKnU9mCfMvTEq54DBLgAh3yYDx\n"
      ],
      "metadata": {
        "id": "AOU-boBLn4Fc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loader = PyPDFLoader\n",
        "PATH = \"/content/YOLOv10_Tutorials.pdf\"\n",
        "loader = Loader(PATH)\n",
        "document = loader.load()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DbRopR4Wnc0x"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding documents into vectors"
      ],
      "metadata": {
        "id": "zwgOlSIfoURM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name='bkai-foundation-models/vietnamese-bi-encoder'\n",
        "    # intfloat/multilingual-e5-base\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7f5rjQXuoTIH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split text by semantic"
      ],
      "metadata": {
        "id": "VmvvbZtMp_v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_splitter = SemanticChunker(\n",
        "    embeddings = embedding,\n",
        "    breakpoint_threshold_type='percentile',\n",
        "    breakpoint_threshold_amount=95,\n",
        "    min_chunk_size=500,\n",
        "    add_start_index = True\n",
        ")"
      ],
      "metadata": {
        "id": "v2wnwCJPpVYg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = semantic_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "30TT1DgIqjXa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVChJ2VFrCbD",
        "outputId": "e0a8c936-78a0-4ab5-c087-3b47cfa5de66"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7twpWseFrXTw",
        "outputId": "f6636065-402c-43d8-fd52-1a4047dab567"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create vector database"
      ],
      "metadata": {
        "id": "ul069bl6rIJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding\n",
        ")"
      ],
      "metadata": {
        "id": "hUaT5rOfrEYD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver = vector_db.as_retriever()\n",
        "prompt = 'YOLOv10 dùng để làm gì ?'\n",
        "results = retriver.invoke(prompt)"
      ],
      "metadata": {
        "id": "61FDmcc7rnlY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PIH9XI8rr3BD",
        "outputId": "7ff760e7-e8c0-4429-c24f-bd198e79c85f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 2: Hiệu suất của YOLOv10 khi so sánh với các mô hình khác. Trên tập dữ liệu COCO,\\nYOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô\\nhình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao. Ảnh: [10]. Trong bài viết này, chúng ta sẽ cùng nhau tìm hiểu về YOLOv10 và cách sử dụng mô hình này. Thông qua đó, nhóm cũng sẽ trình bày sơ lược về bài toán Object Detection cũng như tóm tắt\\nngắn gọn các phiên bản YOLO trước đó để bạn đọc có một cái nhìn tổng quan hơn về nội dung\\nnày. Theo đó, bài viết được bố cục như sau:\\n- Phần I:Giới thiệu về nội dung bài viết. - Phần II:Tóm tắt về bài toán Object Detection và các phiên bản YOLO đời trước. - Phần III:Trình bày nội dung YOLOv10. - Phần IV:Hướng dẫn cách cài đặt, huấn luyện và sử dụng YOLOv10. - Phần V:Trích dẫn tài liệu. 2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use quantization for model vicuna"
      ],
      "metadata": {
        "id": "9fKJpSBwtCTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'lmsys/vicuna-7b-v1.5'\n",
        "\n",
        "# Define the offload folder\n",
        "# offload_folder = \"./offload\"\n",
        "config_nf4 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config = config_nf4,\n",
        "    device_map = 'auto',\n",
        "    low_cpu_mem_usage=True,\n",
        "    # offload_folder=offload_folder # Add offload folder\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    max_new_tokens = 512,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "    device_map ='auto'\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = model_pipeline\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "b75d9936887b4d1d8e20b442659eebde",
            "b906bf27c8c24e97b57d234c3991ca95",
            "28c59e3b317d4e418b9b25b8075f7f2c",
            "ea5cb281e96244f585f2155041e8f95a",
            "bb6e3e23eb9c425fbf0ce5eae5aa1c70",
            "d7855351ddf54e9994e111646239969c",
            "187b7759710b4850a7f76154833af0ea",
            "2e24fa6597794d8192e2bc1cc6d9d829",
            "c99fcc2e6bdb4191b6198a55a5d173fe",
            "ff711eda99564122a60890ec17b2b964",
            "11fe046195d54ae5873328cbc47966a7"
          ]
        },
        "collapsed": true,
        "id": "WBO4xX_juLGv",
        "outputId": "c23f8342-7374-457d-edab-1b3f453642c9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b75d9936887b4d1d8e20b442659eebde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCZim2LrTRoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompting"
      ],
      "metadata": {
        "id": "ok1OJdHZxrFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_promt = hub.pull('rlm/rag-prompt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhtZAAj-r94x",
        "outputId": "8b3146d0-61c4-4fa2-85b1-4df848373a61"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(template_promt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBAyhvtYx2MW",
        "outputId": "8c6f5454-bb93-46e2-aa52-9e641d5eb04e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "  return '\\n\\n'.join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "DZCPoZoEx35q"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {'context' : retriver | format_docs, 'question' : RunnablePassthrough()} # Changed 'content' to 'context'\n",
        "    | template_promt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "4MFLhaC8yS_z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLE_QUESTION = \"YOLOv10 là gì\"\n",
        "output = rag_chain.invoke(EXAMPLE_QUESTION)"
      ],
      "metadata": {
        "id": "I4rTRiUNyuB1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTxtDbNJq6Uy",
        "outputId": "dad9e3a1-1bf8-412b-ba2f-19d0ee27e214"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: YOLOv10 là gì \n",
            "Context: AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 2: Hiệu suất của YOLOv10 khi so sánh với các mô hình khác. Trên tập dữ liệu COCO,\n",
            "YOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô\n",
            "hình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao. Ảnh: [10]. Trong bài viết này, chúng ta sẽ cùng nhau tìm hiểu về YOLOv10 và cách sử dụng mô hình này. Thông qua đó, nhóm cũng sẽ trình bày sơ lược về bài toán Object Detection cũng như tóm tắt\n",
            "ngắn gọn các phiên bản YOLO trước đó để bạn đọc có một cái nhìn tổng quan hơn về nội dung\n",
            "này. Theo đó, bài viết được bố cục như sau:\n",
            "- Phần I:Giới thiệu về nội dung bài viết. - Phần II:Tóm tắt về bài toán Object Detection và các phiên bản YOLO đời trước. - Phần III:Trình bày nội dung YOLOv10. - Phần IV:Hướng dẫn cách cài đặt, huấn luyện và sử dụng YOLOv10. - Phần V:Trích dẫn tài liệu. 2\n",
            "\n",
            "AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 2: Hiệu suất của YOLOv10 khi so sánh với các mô hình khác. Trên tập dữ liệu COCO,\n",
            "YOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô\n",
            "hình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao. Ảnh: [10]. Trong bài viết này, chúng ta sẽ cùng nhau tìm hiểu về YOLOv10 và cách sử dụng mô hình này. Thông qua đó, nhóm cũng sẽ trình bày sơ lược về bài toán Object Detection cũng như tóm tắt\n",
            "ngắn gọn các phiên bản YOLO trước đó để bạn đọc có một cái nhìn tổng quan hơn về nội dung\n",
            "này. Theo đó, bài viết được bố cục như sau:\n",
            "- Phần I:Giới thiệu về nội dung bài viết. - Phần II:Tóm tắt về bài toán Object Detection và các phiên bản YOLO đời trước. - Phần III:Trình bày nội dung YOLOv10. - Phần IV:Hướng dẫn cách cài đặt, huấn luyện và sử dụng YOLOv10. - Phần V:Trích dẫn tài liệu. 2\n",
            "\n",
            "YOLOv9\n",
            "YOLOv9 [9] được giới thiệu vào năm 2024 bởi Chien-Yao Wang, I-Hau Yeh, và Hong-Yuan Mark\n",
            "Liao. Mô hình này cải thiện độ chính xác và tốc độ so với YOLOv8 và giới thiệu nhiều kỹ thuật\n",
            "mới như Programmable Gradient Information (PGI) và Generalized Efficient Layer Aggregation\n",
            "Network (GELAN). 7\n",
            "\n",
            "YOLOv9\n",
            "YOLOv9 [9] được giới thiệu vào năm 2024 bởi Chien-Yao Wang, I-Hau Yeh, và Hong-Yuan Mark\n",
            "Liao. Mô hình này cải thiện độ chính xác và tốc độ so với YOLOv8 và giới thiệu nhiều kỹ thuật\n",
            "mới như Programmable Gradient Information (PGI) và Generalized Efficient Layer Aggregation\n",
            "Network (GELAN). 7 \n",
            "Answer: YOLOv10 là một phiên bản của mô hình máy học dự định để tìm từng hình ảnh trong các tập dữ liệu để xác định và phân loại các đối tượng trong hình ảnh đó. Nó được giới thiệu vào năm 2024 và đã đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô hình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Streamlit deployment"
      ],
      "metadata": {
        "id": "ZiIx6NJPodPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Mục        | Giải thích                                                             |\n",
        "| ---------- | ---------------------------------------------------------------------- |\n",
        "| Mục đích   | Nhận một URL công khai để truy cập Streamlit đang chạy trong notebook  |\n",
        "| `curl`     | Lấy dữ liệu HTTP từ `local.lt`                                         |\n",
        "| `local.lt` | Dịch vụ tạo tunnel, giúp truy cập cổng 8501 (Streamlit) từ bên ngoài   |\n",
        "| Sử dụng    | Khi chạy Streamlit trong môi trường không thể mở cổng như Google Colab |\n"
      ],
      "metadata": {
        "id": "WoIPp_gqqEmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit==1.46.0"
      ],
      "metadata": {
        "id": "qNP07uxSkjBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc85985-93ab-4d25-cccf-5ed6d80af115"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit==1.46.0\n",
            "  Downloading streamlit-1.46.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit==1.46.0)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.46.0)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.46.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.46.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.46.0) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.46.0) (1.45.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.46.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.46.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.46.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.46.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.46.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.46.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.46.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.46.0) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.46.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.46.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit==1.46.0) (1.17.0)\n",
            "Downloading streamlit-1.46.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.46.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D6lWu_n5c0U",
        "outputId": "b31bb81b-27fc-40a2-d364-465163b14743"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.87.77.59"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"HELLO WORLD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id96vWN7rapZ",
        "outputId": "682eccdc-e0ab-4267-e95b-72d91acc4e7d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "MbsU6urkraiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1421a36a-df60-4574-9391-78348d93a517"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.87.77.59:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0Kyour url is: https://cold-ravens-teach.loca.lt\n",
            "2025-07-03 03:11:31.218350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751512291.242322    2553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751512291.249362    2553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-03 03:11:31.276283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 387/387 [00:00<00:00, 1.93MB/s]\n",
            "README.md: 179kB [00:00, 142MB/s]\n",
            "sentence_bert_config.json: 100% 57.0/57.0 [00:00<00:00, 233kB/s]\n",
            "config.json: 100% 694/694 [00:00<00:00, 5.40MB/s]\n",
            "model.safetensors: 100% 1.11G/1.11G [00:04<00:00, 240MB/s]\n",
            "tokenizer_config.json: 100% 418/418 [00:00<00:00, 3.13MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 294MB/s]\n",
            "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 308MB/s]\n",
            "special_tokens_map.json: 100% 280/280 [00:00<00:00, 2.45MB/s]\n",
            "config.json: 100% 200/200 [00:00<00:00, 1.11MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 5.44MB/s]\n",
            "pytorch_model.bin.index.json: 26.8kB [00:00, 98.2MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 10.5M/3.50G [00:00<00:37, 93.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.98G [00:00<01:08, 145MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<00:25, 136MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 41.9M/9.98G [00:00<01:02, 158MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 52.4M/3.50G [00:00<00:25, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/9.98G [00:00<01:17, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 73.4M/3.50G [00:00<00:26, 129MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.98G [00:00<01:21, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 94.4M/3.50G [00:00<00:24, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.98G [00:00<01:18, 125MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 115M/3.50G [00:00<00:23, 144MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.98G [00:00<01:16, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 136M/3.50G [00:00<00:23, 146MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.safetensors.index.json: 28.1kB [00:00, 73.8MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.98G [00:01<01:17, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/3.50G [00:01<00:23, 140MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 178M/3.50G [00:01<00:23, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.98G [00:01<01:17, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 199M/3.50G [00:01<00:22, 147MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 189M/9.98G [00:01<01:14, 131MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 231M/3.50G [00:01<00:19, 167MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.98G [00:01<01:15, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.50G [00:01<00:19, 168MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.98G [00:01<01:12, 134MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 273M/3.50G [00:01<00:19, 166MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 252M/9.98G [00:01<01:12, 135MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 294M/3.50G [00:01<00:20, 160MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.98G [00:02<01:15, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:02<00:21, 146MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.98G [00:02<01:13, 131MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 336M/3.50G [00:02<00:20, 154MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.98G [00:02<01:12, 133MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 357M/3.50G [00:02<00:21, 145MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:02<00:19, 158MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.98G [00:02<01:11, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.98G [00:02<01:18, 123MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 398M/3.50G [00:02<00:24, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.98G [00:02<01:13, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 419M/3.50G [00:02<00:25, 123MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.98G [00:03<01:13, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.50G [00:03<00:23, 131MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:03<01:14, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 461M/3.50G [00:03<00:22, 132MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.98G [00:03<01:11, 133MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 482M/3.50G [00:03<00:23, 130MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.98G [00:03<01:06, 144MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 503M/3.50G [00:03<00:21, 138MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.98G [00:03<01:10, 134MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 524M/3.50G [00:03<00:22, 132MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.98G [00:03<01:19, 120MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 545M/3.50G [00:03<00:24, 121MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:04<01:15, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 566M/3.50G [00:04<00:22, 128MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 587M/3.50G [00:04<00:20, 139MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 545M/9.98G [00:04<01:27, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.98G [00:05<03:21, 46.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 608M/3.50G [00:05<01:02, 46.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 629M/3.50G [00:05<00:48, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:05<02:39, 59.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 650M/3.50G [00:05<00:39, 71.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.98G [00:05<02:10, 72.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 671M/3.50G [00:05<00:33, 83.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.98G [00:05<01:54, 81.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 692M/3.50G [00:05<00:27, 101MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.98G [00:09<09:31, 16.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 713M/3.50G [00:09<02:43, 17.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 682M/9.98G [00:09<06:01, 25.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 744M/3.50G [00:09<01:42, 26.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 703M/9.98G [00:09<04:36, 33.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 765M/3.50G [00:09<01:19, 34.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 724M/9.98G [00:09<03:36, 42.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 786M/3.50G [00:09<01:01, 44.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/9.98G [00:09<02:51, 53.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 807M/3.50G [00:10<00:47, 56.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 828M/3.50G [00:13<02:44, 16.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 765M/9.98G [00:13<09:37, 16.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 849M/3.50G [00:13<01:59, 22.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.98G [00:13<06:11, 24.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 881M/3.50G [00:13<01:16, 34.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.98G [00:13<04:48, 31.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 902M/3.50G [00:13<00:59, 43.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.98G [00:14<03:44, 40.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 923M/3.50G [00:14<00:48, 53.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.98G [00:14<03:01, 50.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 954M/3.50G [00:14<00:33, 74.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.98G [00:14<02:21, 64.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 975M/3.50G [00:14<00:31, 80.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:14<02:07, 71.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 996M/3.50G [00:14<00:28, 87.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.02G/3.50G [00:18<02:41, 15.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 923M/9.98G [00:19<12:31, 12.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/9.98G [00:19<06:56, 21.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.03G/3.50G [00:19<02:55, 14.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 986M/9.98G [00:19<05:32, 27.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.05G/3.50G [00:20<02:02, 19.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.98G [00:20<04:22, 34.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.07G/3.50G [00:20<01:27, 27.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.98G [00:20<03:26, 43.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.10G/3.50G [00:20<00:55, 43.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.12G/3.50G [00:20<00:46, 51.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.05G/9.98G [00:20<02:56, 50.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.14G/3.50G [00:20<00:36, 63.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.98G [00:20<02:21, 63.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.16G/3.50G [00:20<00:31, 74.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.98G [00:20<01:59, 74.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.18G/3.50G [00:20<00:26, 87.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.98G [00:20<01:47, 82.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.21G/3.50G [00:21<00:24, 92.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.98G [00:21<01:32, 95.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.23G/3.50G [00:21<00:21, 103MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:21<01:31, 96.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.25G/3.50G [00:21<00:20, 108MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.17G/9.98G [00:21<01:27, 100MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.27G/3.50G [00:21<00:17, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.98G [00:21<01:22, 107MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.29G/3.50G [00:21<00:20, 110MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:21<01:19, 111MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.31G/3.50G [00:21<00:17, 128MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.33G/3.50G [00:22<00:16, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.24G/9.98G [00:22<01:26, 101MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.35G/3.50G [00:22<00:14, 148MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.98G [00:22<01:15, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:22<01:07, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.37G/3.50G [00:22<00:18, 115MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.30G/9.98G [00:22<01:07, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.39G/3.50G [00:22<00:18, 117MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.98G [00:22<01:07, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.42G/3.50G [00:22<00:16, 130MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/9.98G [00:22<01:05, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.44G/3.50G [00:22<00:16, 124MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.98G [00:23<01:10, 122MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.46G/3.50G [00:23<00:16, 123MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.98G [00:23<01:05, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.48G/3.50G [00:23<00:17, 115MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.98G [00:23<01:06, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.50G/3.50G [00:23<00:17, 116MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.43G/9.98G [00:23<01:04, 133MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.52G/3.50G [00:23<00:16, 121MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.45G/9.98G [00:23<01:02, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.47G/9.98G [00:23<01:06, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.54G/3.50G [00:23<00:19, 102MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.98G [00:23<00:57, 147MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.56G/3.50G [00:24<00:17, 110MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.52G/9.98G [00:24<01:01, 137MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.58G/3.50G [00:24<00:15, 123MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.60G/3.50G [00:24<00:15, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.98G [00:24<01:06, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.98G [00:24<01:10, 119MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.63G/3.50G [00:24<00:16, 114MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:24<01:05, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.65G/3.50G [00:24<00:15, 123MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.67G/3.50G [00:24<00:15, 119MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.98G [00:25<01:24, 99.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.71G/3.50G [00:25<00:11, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:25<01:13, 114MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.73G/3.50G [00:25<00:11, 158MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.65G/9.98G [00:25<01:14, 112MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.75G/3.50G [00:25<00:12, 141MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.77G/3.50G [00:25<00:12, 141MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.98G [00:25<01:16, 108MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.79G/3.50G [00:25<00:11, 144MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.98G [00:25<01:10, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.98G [00:25<01:05, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.81G/3.50G [00:26<00:16, 103MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.98G [00:26<01:24, 98.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.84G/3.50G [00:26<00:15, 105MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.98G [00:26<01:27, 93.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.86G/3.50G [00:26<00:19, 84.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.87G/3.50G [00:26<00:19, 85.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.77G/9.98G [00:26<01:36, 85.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.98G [00:26<01:22, 99.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.90G/3.50G [00:26<00:13, 116MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.93G/3.50G [00:26<00:11, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.98G [00:26<01:16, 107MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.96G/3.50G [00:27<00:09, 161MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.84G/9.98G [00:27<01:11, 115MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.99G/3.50G [00:27<00:08, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.98G [00:27<01:09, 117MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.02G/3.50G [00:27<00:07, 205MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.98G [00:27<01:05, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.06G/3.50G [00:27<00:10, 144MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.91G/9.98G [00:27<01:09, 117MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.08G/3.50G [00:27<00:10, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.93G/9.98G [00:27<01:10, 113MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.10G/3.50G [00:28<00:11, 121MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.98G [00:28<01:19, 101MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.12G/3.50G [00:28<00:11, 118MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/9.98G [00:28<01:03, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.14G/3.50G [00:28<00:10, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/9.98G [00:28<01:05, 122MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.16G/3.50G [00:28<00:09, 138MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.18G/3.50G [00:28<00:08, 153MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/9.98G [00:28<01:03, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.20G/3.50G [00:28<00:09, 138MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.04G/9.98G [00:28<01:05, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.07G/9.98G [00:28<00:58, 136MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.22G/3.50G [00:29<00:09, 137MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.24G/3.50G [00:29<00:08, 151MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/9.98G [00:29<00:55, 142MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.26G/3.50G [00:29<00:09, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.98G [00:29<01:01, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.98G [00:29<01:00, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.29G/3.50G [00:29<00:09, 131MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.31G/3.50G [00:29<00:08, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.15G/9.98G [00:29<00:59, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.98G [00:29<00:56, 139MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.33G/3.50G [00:29<00:08, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/9.98G [00:29<00:56, 139MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.35G/3.50G [00:29<00:08, 140MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.37G/3.50G [00:30<00:08, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/9.98G [00:30<01:00, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.39G/3.50G [00:30<00:07, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.98G [00:30<00:57, 134MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.41G/3.50G [00:30<00:08, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.25G/9.98G [00:30<01:08, 113MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.43G/3.50G [00:30<00:07, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.28G/9.98G [00:30<01:01, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.45G/3.50G [00:30<00:06, 150MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.47G/3.50G [00:30<00:07, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.98G [00:30<01:10, 109MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.50G/3.50G [00:31<00:08, 121MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.98G [00:31<01:12, 105MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.52G/3.50G [00:31<00:08, 113MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/9.98G [00:31<01:16, 99.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.54G/3.50G [00:31<00:09, 104MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.98G [00:31<01:16, 99.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.98G [00:31<01:17, 98.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.56G/3.50G [00:31<00:09, 104MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.98G [00:31<01:10, 107MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.58G/3.50G [00:31<00:08, 111MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/9.98G [00:34<05:08, 24.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.60G/3.50G [00:34<00:34, 26.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.44G/9.98G [00:34<03:14, 38.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.63G/3.50G [00:34<00:21, 40.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.98G [00:34<02:33, 49.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.65G/3.50G [00:34<00:16, 49.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.98G [00:34<01:46, 70.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.67G/3.50G [00:34<00:13, 61.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/9.98G [00:34<01:28, 84.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.69G/3.50G [00:37<00:46, 17.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/9.98G [00:39<09:11, 13.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.71G/3.50G [00:40<01:07, 11.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.98G [00:40<07:43, 16.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.75G/3.50G [00:40<00:33, 22.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.59G/9.98G [00:40<05:03, 24.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.77G/3.50G [00:40<00:25, 28.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.79G/3.50G [00:40<00:19, 37.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.98G [00:40<03:58, 30.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:40<03:03, 40.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.81G/3.50G [00:40<00:14, 46.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.66G/9.98G [00:40<02:09, 56.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.83G/3.50G [00:40<00:11, 56.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.68G/9.98G [00:41<01:49, 66.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.85G/3.50G [00:41<00:09, 67.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/9.98G [00:41<01:35, 76.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.87G/3.50G [00:41<00:08, 76.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/9.98G [00:41<01:19, 91.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.89G/3.50G [00:41<00:06, 90.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.75G/9.98G [00:41<01:16, 94.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.92G/3.50G [00:41<00:06, 89.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.94G/3.50G [00:41<00:05, 102MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.98G [00:41<01:20, 90.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.96G/3.50G [00:42<00:06, 80.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.79G/9.98G [00:42<01:54, 62.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.98G [00:43<03:05, 38.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.98G/3.50G [00:43<00:14, 35.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.81G/9.98G [00:43<03:35, 33.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.00G/3.50G [00:43<00:10, 46.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/9.98G [00:43<02:36, 45.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.02G/3.50G [00:43<00:08, 58.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.85G/9.98G [00:43<02:01, 58.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.04G/3.50G [00:43<00:06, 69.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.87G/9.98G [00:44<01:37, 73.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.06G/3.50G [00:44<00:05, 77.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.98G [00:44<01:43, 68.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.08G/3.50G [00:44<00:07, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.98G [00:44<02:13, 53.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.09G/3.50G [00:44<00:06, 60.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.98G [00:44<01:42, 69.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.11G/3.50G [00:44<00:05, 77.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.95G/9.98G [00:45<01:21, 86.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.14G/3.50G [00:45<00:04, 90.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:45<01:00, 115MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.17G/3.50G [00:45<00:03, 110MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/9.98G [00:45<01:10, 99.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.19G/3.50G [00:45<00:03, 90.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/9.98G [00:45<01:10, 98.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.21G/3.50G [00:45<00:02, 106MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.98G [00:45<00:56, 122MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.23G/3.50G [00:45<00:02, 107MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.07G/9.98G [00:46<00:57, 119MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.25G/3.50G [00:46<00:02, 118MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.10G/9.98G [00:46<00:47, 144MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.28G/3.50G [00:46<00:01, 145MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.98G [00:46<00:49, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.30G/3.50G [00:46<00:01, 119MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.98G [00:46<00:50, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.17G/9.98G [00:46<00:49, 138MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.33G/3.50G [00:46<00:01, 119MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/9.98G [00:46<00:57, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.36G/3.50G [00:46<00:01, 120MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.38G/3.50G [00:47<00:00, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/9.98G [00:47<00:56, 120MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.40G/3.50G [00:47<00:00, 144MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.24G/9.98G [00:47<00:43, 154MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.42G/3.50G [00:47<00:00, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.26G/9.98G [00:47<00:58, 116MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.44G/3.50G [00:47<00:00, 104MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.28G/9.98G [00:47<01:04, 103MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.47G/3.50G [00:47<00:00, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.98G [00:47<00:59, 113MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:47<00:00, 72.9MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/9.98G [00:48<00:55, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.37G/9.98G [00:48<00:45, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.98G [00:48<00:38, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.44G/9.98G [00:48<00:30, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.47G/9.98G [00:48<00:28, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.98G [00:48<00:28, 224MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.53G/9.98G [00:48<00:27, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.98G [00:49<00:28, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.60G/9.98G [00:49<00:26, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.98G [00:49<00:25, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.66G/9.98G [00:49<00:25, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.98G [00:49<00:25, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.98G [00:49<00:35, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.98G [00:49<00:26, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/9.98G [00:50<00:22, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.85G/9.98G [00:50<00:21, 279MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.98G [00:50<00:23, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.91G/9.98G [00:50<00:24, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.94G/9.98G [00:54<04:00, 25.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.98G [00:54<02:40, 37.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.03G/9.98G [00:54<01:52, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.06G/9.98G [00:54<01:30, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.09G/9.98G [00:55<01:11, 82.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.12G/9.98G [00:55<00:57, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.15G/9.98G [00:55<00:47, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.18G/9.98G [00:55<00:40, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.98G [00:55<00:31, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.98G [00:55<00:28, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [00:55<00:25, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.33G/9.98G [00:55<00:24, 226MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [00:56<00:24, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.98G [00:56<00:48, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.41G/9.98G [00:57<01:32, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.44G/9.98G [00:57<01:24, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.48G/9.98G [00:58<00:56, 97.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.52G/9.98G [00:58<00:40, 134MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [00:58<01:00, 89.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.98G [00:58<00:44, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/9.98G [00:59<00:44, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.98G [00:59<00:44, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [00:59<00:45, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.98G [00:59<00:46, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.72G/9.98G [00:59<00:44, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.98G [01:00<00:32, 162MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.98G [01:00<00:26, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.83G/9.98G [01:00<00:25, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.98G [01:00<00:25, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [01:00<00:25, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.94G/9.98G [01:00<00:22, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.98G [01:01<00:32, 156MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.98G [01:04<03:21, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.04G/9.98G [01:04<01:58, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.10G/9.98G [01:04<01:17, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.13G/9.98G [01:05<01:01, 78.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.17G/9.98G [01:05<00:46, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.98G [01:05<00:36, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [01:05<00:32, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.98G [01:05<00:27, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.32G/9.98G [01:05<00:22, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.98G [01:05<00:20, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.98G [01:05<00:17, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.43G/9.98G [01:06<00:18, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.98G [01:06<00:16, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [01:06<00:17, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.54G/9.98G [01:06<00:17, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.98G [01:06<00:22, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.98G [01:06<00:23, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.98G [01:07<00:21, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.98G [01:07<00:20, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.70G/9.98G [01:07<00:20, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.75G/9.98G [01:07<00:18, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.98G [01:07<00:18, 228MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.98G [01:07<00:16, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.98G [01:07<00:16, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.98G [01:08<00:16, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.98G [01:08<00:15, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.98G [01:08<00:15, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.98G [01:08<00:16, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.98G [01:08<00:16, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.98G [01:08<00:14, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.08G/9.98G [01:08<00:14, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.11G/9.98G [01:08<00:14, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.14G/9.98G [01:09<00:14, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.18G/9.98G [01:09<00:20, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.22G/9.98G [01:09<00:16, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.98G [01:09<00:15, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.28G/9.98G [01:09<00:16, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [01:09<00:16, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.35G/9.98G [01:10<00:14, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.40G/9.98G [01:10<00:13, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.43G/9.98G [01:10<00:13, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.46G/9.98G [01:10<00:18, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.49G/9.98G [01:15<02:41, 21.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.98G [01:15<02:18, 25.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.54G/9.98G [01:21<04:47, 11.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.98G [01:21<03:22, 16.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.61G/9.98G [01:21<02:23, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.65G/9.98G [01:21<01:33, 35.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.67G/9.98G [01:21<01:17, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.70G/9.98G [01:21<00:56, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.74G/9.98G [01:22<00:38, 85.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.98G [01:22<00:30, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.81G/9.98G [01:22<00:24, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.84G/9.98G [01:22<00:20, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.87G/9.98G [01:22<00:18, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.98G [01:22<00:16, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.93G/9.98G [01:22<00:15, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.96G/9.98G [01:22<00:13, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.99G/9.98G [01:23<00:12, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.03G/9.98G [01:23<00:13, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.06G/9.98G [01:23<00:14, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.09G/9.98G [01:23<00:12, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.13G/9.98G [01:23<00:11, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.98G [01:23<00:10, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.98G [01:23<00:10, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.98G [01:24<00:12, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.98G [01:24<00:10, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.32G/9.98G [01:24<00:10, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.98G [01:24<00:10, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [01:24<00:11, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.42G/9.98G [01:24<00:09, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.46G/9.98G [01:24<00:09, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.50G/9.98G [01:25<00:08, 283MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.53G/9.98G [01:25<00:10, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.98G [01:25<00:09, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.61G/9.98G [01:25<00:08, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.98G [01:25<00:08, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.98G [01:25<00:09, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.72G/9.98G [01:25<00:08, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.98G [01:26<00:08, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.98G [01:26<00:08, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.81G/9.98G [01:26<00:08, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.84G/9.98G [01:26<00:08, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.87G/9.98G [01:26<00:08, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.91G/9.98G [01:26<00:08, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.98G [01:26<00:08, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.97G/9.98G [01:27<00:08, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.00G/9.98G [01:27<00:17, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [01:27<00:12, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [01:31<01:14, 25.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.10G/9.98G [01:31<01:00, 31.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.12G/9.98G [01:31<00:48, 38.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.16G/9.98G [01:31<00:30, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.19G/9.98G [01:32<00:23, 76.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.22G/9.98G [01:32<00:17, 98.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.25G/9.98G [01:32<00:14, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.28G/9.98G [01:32<00:11, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.32G/9.98G [01:32<00:09, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.35G/9.98G [01:32<00:08, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.38G/9.98G [01:32<00:07, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.41G/9.98G [01:32<00:06, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.44G/9.98G [01:33<00:06, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.98G [01:33<00:06, 221MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [01:33<00:05, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.55G/9.98G [01:33<00:05, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.58G/9.98G [01:33<00:05, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.61G/9.98G [01:33<00:05, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [01:33<00:05, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [01:33<00:04, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.70G/9.98G [01:34<00:04, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [01:34<00:04, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.77G/9.98G [01:34<00:04, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [01:34<00:04, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.83G/9.98G [01:34<00:04, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.86G/9.98G [01:34<00:04, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.98G [01:34<00:04, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.98G [01:34<00:04, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.98G [01:35<00:03, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.00G/9.98G [01:35<00:03, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.03G/9.98G [01:35<00:03, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.06G/9.98G [01:35<00:03, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.10G/9.98G [01:35<00:03, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.13G/9.98G [01:35<00:03, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.16G/9.98G [01:35<00:03, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.20G/9.98G [01:35<00:03, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.23G/9.98G [01:36<00:03, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.98G [01:36<00:02, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [01:36<00:03, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.98G [01:36<00:03, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.98G [01:36<00:02, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.42G/9.98G [01:36<00:01, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.98G [01:36<00:01, 283MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.49G/9.98G [01:37<00:01, 307MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.53G/9.98G [01:37<00:01, 291MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.98G [01:37<00:01, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.60G/9.98G [01:37<00:01, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.98G [01:37<00:01, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.98G [01:37<00:01, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.70G/9.98G [01:37<00:01, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.98G [01:38<00:01, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.76G/9.98G [01:38<00:01, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.98G [01:38<00:01, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.81G/9.98G [01:38<00:00, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.85G/9.98G [01:38<00:00, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.98G [01:39<00:00, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.91G/9.98G [01:39<00:00, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.94G/9.98G [01:39<00:00, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [01:39<00:00, 100MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:39<00:00, 49.86s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [01:07<00:00, 33.56s/it]\n",
            "generation_config.json: 100% 162/162 [00:00<00:00, 1.20MB/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "tokenizer_config.json: 100% 749/749 [00:00<00:00, 5.34MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 24.4MB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 4.22MB/s]\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "if 'models_loaded' not in st.session_state:\n",
        "  st.session_state.models_loaded = None\n",
        "\n",
        "if 'embedding' not in st.session_state:\n",
        "  st.session_state.embedding = None\n",
        "\n",
        "if 'llm' not in st.session_state:\n",
        "  st.session_state.llm = None\n",
        "\n",
        "#dùng để tránh gọi nhiều lần, đỡ tồn tài nguyên, cache sẵn\n",
        "#chỉ có tác dụng lên hàm ngay dưới đó\n",
        "@st.cache_resource\n",
        "def load_embedding():\n",
        "  return HuggingFaceEmbeddings(\n",
        "      model = 'intfloat/multilingual-e5-base'\n",
        "  )\n",
        "\n",
        "def load_llm():\n",
        "\n",
        "  MODEL_NAME = 'lmsys/vicuna-7b-v1.5'\n",
        "\n",
        "  config_nf4 = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype= torch.float16,\n",
        "      bnb_4bit_quant_type='nf4',\n",
        "      bnb_4bit_use_double_quant=True\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      MODEL_NAME,\n",
        "      quantization_config = config_nf4,\n",
        "      low_cpu_mem_usage = True,\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "  model_pipeline = pipeline(\n",
        "      'text-generation',\n",
        "      model = model,\n",
        "      max_new_tokens = 512,\n",
        "      pad_token_id = tokenizer.eos_token_id,\n",
        "      tokenizer = tokenizer,\n",
        "      device_map = 'auto'\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(\n",
        "      pipeline = model_pipeline\n",
        "  )\n",
        "\n",
        "  return llm\n",
        "\n",
        "def process_pdf(file_uploaded):\n",
        "  with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "    tmp_file.write(file_uploaded.getvalue())\n",
        "    tmp_file_path = tmp_file.name\n",
        "\n",
        "  Loader = PyPDFLoader\n",
        "  loader = Loader(tmp_file_path)\n",
        "  document = loader.load()\n",
        "\n",
        "  semantic_splitter = SemanticChunker(\n",
        "      embeddings = st.session_state.embedding,\n",
        "      add_start_index = True,\n",
        "      buffer_size=1,\n",
        "      breakpoint_threshold_type=\"percentile\",\n",
        "      breakpoint_threshold_amount=95,\n",
        "      min_chunk_size=500\n",
        "  )\n",
        "\n",
        "  docs = semantic_splitter.split_documents(document)\n",
        "  vector_db = Chroma.from_documents(documents = docs, embedding=st.session_state.embedding)\n",
        "  retriver = vector_db.as_retriever()\n",
        "\n",
        "  prompt = hub.pull('rlm/rag-prompt')\n",
        "\n",
        "  def format_docs(docs):\n",
        "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
        "\n",
        "  rag_chain = (\n",
        "      {'context': retriver|format_docs, 'question': RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | st.session_state.llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  os.unlink(tmp_file_path)\n",
        "  return rag_chain, len(docs)\n",
        "\n",
        "\n",
        "def main():\n",
        "  st.set_page_config(page_title=\"PDF RAG Assistance\", layout='wide')\n",
        "  st.title(\"PDF RAG Assistant\")\n",
        "\n",
        "  st.markdown(\"\"\"\n",
        "  **Ứng dụng AI giúp bạn hỏi đáp trực tiếp với nội dung tài liệu PDF bằng tiếng Việt**\n",
        "\n",
        "  **Cách sử dụng đơn giản:**\n",
        "  1. **Upload PDF** → Chọn file PDF từ máy tính và nhấn \"Xử lý PDF\"\n",
        "  2. **Đặt câu hỏi** → Nhập câu hỏi về nội dung tài liệu và nhận câu trả lời ngay lập tức\n",
        "\n",
        "  ---\n",
        "  \"\"\")\n",
        "\n",
        "  if not st.session_state.models_loaded:\n",
        "    st.info(\"Loading models\")\n",
        "    st.session_state.embedding = load_embedding()\n",
        "    st.session_state.llm = load_llm()\n",
        "    st.session_state.models_loaded = True\n",
        "    st.success(\"Model Preparation is Done!\")\n",
        "    st.rerun()\n",
        "\n",
        "  uploaded_file = st.file_uploader(\"Upload your PDF file\", type='pdf')\n",
        "  if uploaded_file and st.button(\"Press to process file\"):\n",
        "    st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)\n",
        "    st.success(f\"The semantic split on file is done. Number of chunks is {num_chunks}\")\n",
        "\n",
        "  question = st.text_input(\"Please input a question\")\n",
        "  if question:\n",
        "    with st.spinner(\"Answering question...\"):\n",
        "        output = st.session_state.rag_chain.invoke(question)\n",
        "        answer = output.split(\"Answer: \")[1] if \"Answer: \" in output else output\n",
        "        st.write(\"The answer is:\")\n",
        "        st.write(answer)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "id": "0gP9wgzLraFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1066f93d-c38a-47a1-d9c5-edc698a4926d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BS8gvKg5HZ33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}